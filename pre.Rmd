---
title: "Plug and Play Priors for Model Based Reconstruction"
author: "Zhengyang Fang, Jin Li"
date: "May 8, 2019"
output: slidy_presentation
---

## Motivation

- In class, we had an image reconstruction problem: 

$y$: observation. $x$: the "truth". $A$: forward transform.

$$
\hat x =\arg \min_x\|y-Ax\|_2^2-\tau\|x\|_{TV}.
$$

Solve by proximal gradient method:

Step 1: Gradient Step:
$$
\tilde x=x^{(k)}-t_kA^T(y-Ax^{(k)}).
$$
Step 2: Solve:
$$
x^{(k+1)}=\arg\min_x \frac12\|x-\tilde x\|_2^2+t_k\tau\|x\|_{TV}.
$$

- Motivation

    + Generalize those two terms

    + Use ADMM instead of proximal gradient method

    + Plug-and-play: apply *(any)* denoising algorithm



## Why the Plug and Play Model Is Important


- In the previous slide, solving the forward (the forwards transform) plus prior (the denoising part) model is specific to the problem

- It is sub optimal to create a new algorithm every time we have a different combination of a forward and prior model

- The goal of the Plug-and-Play Priors for model reconstruction aims to solve that issue


## Brief Background 

- There has been great progress in improving forward models and denoising algorithms, but little progress has been made to cleanly integrate those two steps

- There has been attempts (ex. BM3D) to incorporate advanced priors with general inverse problems, but is not general enough

## What the Model Provides



- this model provides the flexibility to combine state of the art forward models with state of the art denoising models

- allows us to use denoising methods that are not explicitly formulated as an optimization problem

- simplifies software integration by decoupling the prior and forwards model terms


## Algorithm (1)

- Intro to ADMM

$$
\begin{aligned}
x\in \mathbb R^m&,z\in \mathbb R^n, c\in \mathbb R^p.\\ \\
\min_{x,z}&~f(x)+g(z),\\
s.t.&~Ax+Bz=c.
\end{aligned}
$$

Augmented Lagrangian ("soft" constraint).
$$
\min_{x,z,y}L_\rho(x,z,y)=f(x)+g(z)+y^T(Ax+Bz-c)+\frac\rho2\|Ax+Bz-c\|_2^2.
$$

## Algorithm (2)

- MAP (maximal a posteriori)

$$
\max_x f(x|y)\propto f(y|x)f(x),
$$
which is equivalent to maximize
$$
\max_x \log f(y|x)+\log f(x)=l(y;x)+\beta \cdot s(x).
$$

## Algorithm (3)

- Framework in this paper

Solve
$$
\begin{aligned}
(\hat x,\hat v)=~&\arg\max_{x,v}\{l(y;x)+\beta\cdot s(v)\},\\
&s.t.~x=v.
\end{aligned}
$$

$l(y;x)$: the forward transform.

$\beta\cdot s(v)$: the prior.

Algorithm:

> repeat until converge:
$$
\begin{aligned}
&\hat x\leftarrow \arg\min_x\left\{l(y;x)+\frac\lambda 2\|x-(\hat v-u)\|_2^2\right\},\\
&\hat v\leftarrow \arg\min_v\left\{\frac \lambda 2\|v-(\hat x +u)\|^2_2+\beta s(v)\right\},\\
&v\leftarrow u+(\hat x-\hat v).
\end{aligned}
$$



## Experiment and recent work (optional)

optional


## More about ADMM (optional)

- dual descent
- Multiplier method 
- ADMM

